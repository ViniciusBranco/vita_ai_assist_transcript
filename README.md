# [Vita.AI](http://vita.ai/) - Assistente de Prontu√°rio Inteligente

O [**Vita.AI**](http://vita.ai/) √© uma solu√ß√£o de Intelig√™ncia Artificial Generativa local projetada para cl√≠nicas m√©dicas e odontol√≥gicas. O sistema atua como um assistente virtual no WhatsApp, recebendo √°udios de consultas, transcrevendo-os e estruturando automaticamente os dados em prontu√°rios cl√≠nicos (Anamnese e Evolu√ß√£o) para revis√£o posterior via Interface Web.

![Status](https://img.shields.io/badge/Status-MVP%20Completed-success)

![Stack](https://img.shields.io/badge/AI-Local%20LLM%20(Qwen)-blue)
![Stack](https://img.shields.io/badge/LangChain-ffffff?logo=langchain&logoColor=green)


## üöÄ Funcionalidades Principais

- **Transcri√ß√£o de Voz (ASR):** Utiliza *Faster-Whisper* rodando localmente (CPU/GPU) para converter √°udios do WhatsApp em texto.
- **Intelig√™ncia Cl√≠nica (Agentic AI):** Utiliza *LangGraph* e *Qwen 2.5 7B* para analisar o texto, separar "Anamnese" de "Evolu√ß√£o" e extrair procedimentos t√©cnicos estruturados.
- **Fluxo WhatsApp:** Integra√ß√£o via *WAHA (WhatsApp HTTP API)* para receber √°udios e notificar o profissional.
- **Persist√™ncia:** Armazenamento relacional com *PostgreSQL* (dados JSONB para flexibilidade de schema).
- **Interface de Revis√£o:** Frontend *React + Tailwind v4* para que o m√©dico valide e edite o prontu√°rio gerado pela IA.

## üèóÔ∏è Arquitetura T√©cnica

O projeto segue uma arquitetura de microsservi√ßos via Docker Compose:

1. **Backend (FastAPI):** Orquestrador, API REST e Webhooks.
2. **Frontend (Vite/React):** UI para upload manual e revis√£o de prontu√°rios (`/record/:id`).
3. **AI Engine (Ollama):** Servidor de infer√™ncia rodando Qwen 2.5 7B (GPU Passthrough).
4. **Database (PostgreSQL):** Persist√™ncia de dados.
5. **Gateway (WAHA):** Conex√£o com a rede do WhatsApp.

## üõ†Ô∏è Requisitos de Hardware (Local)

- **GPU:** NVIDIA com no m√≠nimo 6GB VRAM (Recomendado: GTX 1060 ou superior/T4 em Cloud).
- **RAM:** 16GB+ (Recomendado 32GB+ para rodar Docker + Ollama confortavelmente).
- **Disk:** SSD com ~20GB livres para imagens Docker e Modelos LLM.

## üì¶ Instala√ß√£o e Execu√ß√£o

### 1. Configura√ß√£o Inicial

Clone o reposit√≥rio e crie o arquivo de vari√°veis de ambiente:

```bash
cp .env.example .env
# Edite o .env com suas configura√ß√µes (WAHA_API_KEY, etc)
```

### 2. Preparar Modelos de IA

√â necess√°rio ter o **Ollama** instalado no host ou usar o container dedicado. Baixe os modelos necess√°rios:

```bash
# No host ou dentro do container Ollama
ollama pull qwen2.5:7b
```

**3. Executar com Docker Compose**

```bash
# Build e Start (Modo Detached)
docker-compose up --build -d
```

O sistema estar√° dispon√≠vel em:

- **Frontend:** http://localhost:5173
- **Backend API:** http://localhost:8000/docs
- **WAHA Dashboard:** http://localhost:3000/dashboard

## üß™ Como Testar (Simula√ß√£o)

Para validar o fluxo sem conectar um WhatsApp real imediatamente:

1. Inicie um servidor de arquivos na raiz para servir o √°udio de teste:

```bash
python -m http.server 9000
```

1. Execute o script de simula√ß√£o de Webhook

```bash
python test_webhook_simulation.py
```

1. Acesse o link gerado nos logs para visualizar o prontu√°rio no Frontend.

## üõ°Ô∏è Seguran√ßa e Privacidade

- **100% Local:** Nenhum √°udio ou texto √© enviado para APIs externas (OpenAI/Anthropic). Tudo roda na sua infraestrutura.
- **Isolamento:** Containers Docker em rede interna.

## üìú Licen√ßa

Propriet√°rio.